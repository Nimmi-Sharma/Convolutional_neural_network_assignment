{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-20 17:01:55.853995: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-20 17:01:55.901085: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-20 17:01:55.902278: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-20 17:01:57.202256: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import PIL\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_train = pathlib.Path(\"/Users/nimmisharma/Downloads/Skin cancer ISIC The International Skin Imaging Collaboration/Train\")\n",
    "data_dir_test = pathlib.Path(\"/Users/nimmisharma/Downloads/Skin cancer ISIC The International Skin Imaging Collaboration/Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "image_count_train = len(list(data_dir_train.glob('*/*.jpg')))\n",
    "print(image_count_train)\n",
    "image_count_test = len(list(data_dir_test.glob('*/*.jpg')))\n",
    "print(image_count_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load using keras.preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 32\n",
    "img_height = 180\n",
    "img_width = 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m## Use 80% of the images for training, and 20% for validation.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[39m## Write your train dataset here\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m## Note use seed=123 while creating your dataset using tf.keras.preprocessing.image_dataset_from_directory\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m## Note, make sure your resize your images to the size img_height*img_width, while writting the dataset\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m train_ds \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(x_train\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], size\u001b[39m=\u001b[39m\u001b[39m2249\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[39m## Write your validation dataset here\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39m## Note use seed=123 while creating your dataset using tf.keras.preprocessing.image_dataset_from_directory\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39m## Note, make sure your resize your images to the size img_height*img_width, while writting the dataset\u001b[39;00m\n\u001b[1;32m     13\u001b[0m val_ds \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(x_train\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], size\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "## Use 80% of the images for training, and 20% for validation.\n",
    "\n",
    "\n",
    "## Write your train dataset here\n",
    "## Note use seed=123 while creating your dataset using tf.keras.preprocessing.image_dataset_from_directory\n",
    "## Note, make sure your resize your images to the size img_height*img_width, while writting the dataset\n",
    "train_ds = np.random.randint(x_train.shape[0], size=2249)\n",
    "     \n",
    "\n",
    "## Write your validation dataset here\n",
    "## Note use seed=123 while creating your dataset using tf.keras.preprocessing.image_dataset_from_directory\n",
    "## Note, make sure your resize your images to the size img_height*img_width, while writting the dataset\n",
    "val_ds = np.random.randint(x_train.shape[0], size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m class_names \u001b[39m=\u001b[39m train_ds\u001b[39m.\u001b[39mclass_names\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(class_names)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_ds' is not defined"
     ]
    }
   ],
   "source": [
    "class_names = train_ds.class_names\n",
    "print(class_names)\n",
    "     "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compile the model\n",
    "### Todo, choose an appropirate optimiser and loss function\n",
    "model.compile(optimizer='your_optimser',\n",
    "              loss=your_loss_function_goes_here,\n",
    "              metrics=['accuracy'])\n",
    "     \n",
    "\n",
    "# View the summary of all layers\n",
    "model.summary()\n",
    "     "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising Training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
